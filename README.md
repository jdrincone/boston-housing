# Boston Housing Price Prediction API

Pipeline **MLOps end-to-end** para entrenar y desplegar un modelo de **regresiÃ³n** sobre el dataset de **Boston Housing**. Incluye versionado de datos con **DVC**, entrenamiento reproducible con **AutoML (FLAML)**, artefactos versionados y una **API** en **FastAPI** lista para producciÃ³n con base de datos PostgreSQL.

---

## ğŸš€ TecnologÃ­as

- **API**: FastAPI + Uvicorn  
- **Base de datos**: PostgreSQL + SQLAlchemy
- **ContenerizaciÃ³n**: Docker + Docker Compose
- **Versionado**: Git + DVC  
- **GestiÃ³n de entorno**: `uv` (wrapper de pip/venv)  
- **Modelado**: scikit-learn, FLAML (AutoML), SHAP, XGBoost
- **Testing**: pytest + httpx
- **Linting**: ruff

---

## âš™ï¸ Prerrequisitos

- Python **3.11+**
- Git
- Docker & Docker Compose
- `uv` â†’ `pip install uv`
- (Opcional) DVC remoto configurado si vas a `dvc pull` desde storage

---

## ğŸ“¦ Clonar y configurar

```bash
git clone https://github.com/jdrincone/boston-housing.git
cd boston-housing

# Crear y activar entorno
uv venv
source .venv/bin/activate          # Linux/macOS
# .venv\Scripts\activate           # Windows

# Instalar dependencias
uv pip install -r requirements.txt
```

---

## ğŸ§  Pipeline de entrenamiento

Los datos y artefactos se versionan con DVC.

### 1) Descargar datos con DVC
```bash
dvc pull
```
Si es la primera vez, asegÃºrate de tener configurado el remoto de DVC (S3).

### 2) Reproducir el pipeline completo
```bash
dvc repro
```

Al finalizar, tendrÃ¡s:
- **Modelo**: `models/best_pipeline.pkl`
- **Reportes**: `reports/` (mÃ©tricas, SHAP plots, feature importance)
- **MÃ©tricas**: `reports/metrics.json`
- **Logs**: `reports/main.log`

Para ver el grafo del pipeline: `dvc dag`

### 3) Reentreno del modelo
```bash
dvc repro --force
```

---

## ğŸ—ï¸ Arquitectura del proyecto

```
boston-housing/
â”œâ”€â”€ app/                          # API FastAPI
â”‚   â”œâ”€â”€ main.py                   # AplicaciÃ³n principal con endpoints
â”‚   â”œâ”€â”€ schemas.py                # Modelos Pydantic para validaciÃ³n
â”‚   â””â”€â”€ database.py               # ConfiguraciÃ³n de PostgreSQL
â”œâ”€â”€ src/                          # CÃ³digo fuente del ML pipeline
â”‚   â”œâ”€â”€ config.py                 # ConfiguraciÃ³n y rutas
â”‚   â”œâ”€â”€ data_manager.py           # I/O de modelos y mÃ©tricas
â”‚   â”œâ”€â”€ pipeline.py               # Pipeline de ML con FLAML
â”‚   â””â”€â”€ train.py                  # Script de entrenamiento
â”œâ”€â”€ scripts/                      # Scripts de preparaciÃ³n
â”‚   â””â”€â”€ prepare_data.py           # DivisiÃ³n train/backtest
|  â””â”€â”€ backtesting.py               # Evaluar el rendimiento del modelo
â”œâ”€â”€ data/                         # Datos (DVC tracked)
â”‚   â”œâ”€â”€ HousingData.csv           # Dataset original
â”‚   â”œâ”€â”€ train_data.csv            # Datos de entrenamiento
â”‚   â””â”€â”€ backtest_data.csv         # Datos de backtesting
â”œâ”€â”€ models/                       # Modelos entrenados (DVC tracked)
â”‚   â””â”€â”€ best_pipeline.pkl         # Pipeline completo serializado
â”œâ”€â”€ reports/                      # Reportes y mÃ©tricas (DVC tracked)
â”‚   â”œâ”€â”€ metrics.json              # MÃ©tricas del modelo
â”‚   â”œâ”€â”€ shap_summary.png          # AnÃ¡lisis SHAP
â”‚   â”œâ”€â”€ feature_importance.png    # Importancia de features
â”‚   â”œâ”€â”€ automl_summary.txt        # Resumen de AutoML
â”‚   â””â”€â”€ main.log                  # Logs de entrenamiento
â”œâ”€â”€ tests/                        # Tests unitarios
â”‚   â”œâ”€â”€ test_api.py               # Tests de la API
â”‚   â””â”€â”€ test_training.py          # Tests del pipeline
â”œâ”€â”€ dvc.yaml                      # ConfiguraciÃ³n del pipeline DVC
â”œâ”€â”€ params.yaml                   # ParÃ¡metros del modelo
â”œâ”€â”€ docker-compose.yml            # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile                    # Imagen de la API
â””â”€â”€ requirements.txt              # Dependencias Python
```

---

## ğŸš€ Despliegue con Docker

### Levantar los servicios completos
```bash
docker-compose up --build
```

Este comando:
- Construye la imagen de la API
- Levanta PostgreSQL en el puerto 5432
- Levanta la API en el puerto 8000
- Configura la conexiÃ³n entre servicios

### Acceder a la API
- **API**: http://localhost:8000
- **DocumentaciÃ³n**: http://localhost:8000/docs
- **Base de datos**: localhost:5432

---
## ğŸ“Š CaracterÃ­sticas del modelo

### Features utilizadas
- **CRIM**: Tasa de criminalidad per cÃ¡pita
- **ZN**: ProporciÃ³n de terreno residencial zonificado
- **INDUS**: ProporciÃ³n de acres de negocio no minorista
- **CHAS**: Variable dummy del rÃ­o Charles
- **NOX**: ConcentraciÃ³n de Ã³xidos nÃ­tricos
- **RM**: NÃºmero promedio de habitaciones por vivienda
- **AGE**: ProporciÃ³n de unidades ocupadas construidas antes de 1940
- **DIS**: Distancias ponderadas a cinco centros de empleo de Boston
- **RAD**: Ãndice de accesibilidad a autopistas radiales
- **TAX**: Tasa de impuesto a la propiedad
- **PTRATIO**: Ratio alumno-profesor por ciudad
- **B**: ProporciÃ³n de afroamericanos por ciudad
- **LSTAT**: % de estatus socioeconÃ³mico bajo

### Pipeline de ML
1. **ImputaciÃ³n**: Valores faltantes con mediana
2. **Escalado**: StandardScaler
3. **AutoML**: FLAML con bÃºsqueda automÃ¡tica de hiperparÃ¡metros
4. **MÃ©tricas**: RÂ², MSE, RMSE, MAE
5. **Explicabilidad**: SHAP y feature importance

---


### Health Check
```bash
curl http://localhost:8000/
```

### PredicciÃ³n
```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{
       "CRIM": 0.02731,
       "ZN": 0.0,
       "INDUS": 7.07,
       "CHAS": 0,
       "NOX": 0.469,
       "RM": 6.421,
       "AGE": 78.9,
       "DIS": 4.9671,
       "RAD": 2,
       "TAX": 242,
       "PTRATIO": 17.8,
       "B": 396.9,
       "LSTAT": 9.14
     }'
```

### Respuesta
```json
{
  "prediction": 24.5
}
```

---

## ğŸ“ˆ Monitoreo y logs

- **Logs de entrenamiento**: `reports/main.log`
- **MÃ©tricas del modelo**: `reports/metrics.json`
- **AnÃ¡lisis SHAP**: `reports/shap_summary.png`
- **Importancia de features**: `reports/feature_importance.png`
- **Resumen AutoML**: `reports/automl_summary.txt`

---

## ğŸ§ª Testing

```bash
# Ejecutar todos los tests
pytest

# Tests especÃ­ficos
pytest tests/test_api.py
pytest tests/test_training.py

```

---

## ğŸ”„ CI/CD Pipeline

El proyecto incluye un pipeline de **CI/CD** automatizado con **GitHub Actions** que se ejecuta en cada push y pull request.

### ConfiguraciÃ³n del Pipeline

El archivo `.github/workflows/ci.yml` define el pipeline que incluye:

#### **Triggers**
- **Push** a las ramas `main` y `develop`
- **Pull Requests** hacia la rama `main`

#### **Servicios**
- **PostgreSQL 13**: Base de datos de prueba con health checks
- **Ubuntu Latest**: Sistema operativo del runner

#### **Pasos del Pipeline**

1. **Checkout del cÃ³digo**
   ```yaml
   - Checkout repository (actions/checkout@v4)
   ```

2. **ConfiguraciÃ³n del entorno Python**
   ```yaml
   - Set up Python 3.11 (actions/setup-python@v5)
   - Install uv and dependencies
   ```

3. **ConfiguraciÃ³n de AWS para DVC**
   ```yaml
   - Configure AWS Credentials (aws-actions/configure-aws-credentials@v4)
   - Pull DVC tracked data
   ```

4. **Linting y validaciÃ³n**
   ```yaml
   - Lint code with Ruff
   ```

5. **Testing**
   ```yaml
   - Run Tests with Pytest (con PostgreSQL de prueba)
   ```

### Variables de Entorno Requeridas

Para que el pipeline funcione correctamente, se necesitan configurar los siguientes **secrets** en GitHub:

- `AWS_ACCESS_KEY_ID`: Clave de acceso de AWS para DVC
- `AWS_SECRET_ACCESS_KEY`: Clave secreta de AWS para DVC

### ConfiguraciÃ³n de Secrets

1. Ve a **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Agrega los secrets necesarios:
   - `AWS_ACCESS_KEY_ID`
   - `AWS_SECRET_ACCESS_KEY`

### Estado del Pipeline

El pipeline valida:
- âœ… **Linting**: CÃ³digo limpio con Ruff
- âœ… **Tests**: Todos los tests unitarios pasan
- âœ… **DVC**: Datos y modelos se descargan correctamente
- âœ… **Base de datos**: ConexiÃ³n a PostgreSQL funcional

---

## ğŸ“Š Backtesting del Modelo

El proyecto incluye un sistema de **backtesting** que permite evaluar el rendimiento del modelo en datos no vistos mediante llamadas a la API en producciÃ³n.

### Funcionalidad del Backtesting

El script `scripts/backtesting.py` realiza las siguientes operaciones:

1. **Carga de datos**: Lee el archivo `data/backtest_data.csv` (5% de los datos originales)
2. **Llamadas a la API**: EnvÃ­a cada registro a `http://localhost:8000/predict`
3. **ComparaciÃ³n**: Compara predicciones vs valores reales
4. **Reporte**: Genera un informe detallado con mÃ©tricas

### Ejecutar Backtesting

```bash
# AsegÃºrate de que la API estÃ© corriendo
docker-compose up --build

# En otra terminal, ejecuta el backtesting
python scripts/backtesting.py
```

### Archivos Generados

- **`reports/backtest_report.csv`**: Reporte detallado con predicciones vs valores reales
- **`reports/backtest.log`**: Logs del proceso de backtesting

### Estructura del Reporte

El archivo `backtest_report.csv` contiene:
- `id`: Identificador del registro
- `actual_value`: Valor real del precio de la vivienda
- `predicted_value`: PredicciÃ³n del modelo
- `payload_sent`: Datos enviados a la API
- `error`: Errores si los hay (ej: valores fuera de rango JSON)


### ValidaciÃ³n de la API

El backtesting valida:
- âœ… **Conectividad**: La API responde correctamente
- âœ… **Formato de datos**: ValidaciÃ³n de esquemas Pydantic
- âœ… **Predicciones**: Valores numÃ©ricos vÃ¡lidos
- âœ… **Manejo de errores**: Valores fuera de rango JSON
- âœ… **Logging**: Registro detallado de cada operaciÃ³n